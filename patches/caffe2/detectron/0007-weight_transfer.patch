From 47cde9eab95a1be7c2d0f2b4ae2015245459445a Mon Sep 17 00:00:00 2001
From: Julien CHICHA <julien.chicha@epitech.eu>
Date: Wed, 23 Jan 2019 19:16:11 +0100
Subject: [PATCH 7/7] weight_transfer

---
 detectron/modeling/detector.py        |   2 +-
 detectron/modeling/mask_rcnn_heads.py | 231 +++++++++++++++++++++-----
 detectron/modeling/model_builder.py   |  17 +-
 detectron/modeling/rpn_heads.py       |   4 +-
 4 files changed, 206 insertions(+), 48 deletions(-)

diff --git a/detectron/modeling/detector.py b/detectron/modeling/detector.py
index 1dcd9ed..08e1269 100644
--- a/detectron/modeling/detector.py
+++ b/detectron/modeling/detector.py
@@ -298,7 +298,7 @@ class DetectionModelHelper(cnn.CNNModelHelper):
                 sampling_ratio=sampling_ratio
             )
         # Only return the first blob (the transformed features)
-        return xform_out[0] if isinstance(xform_out, tuple) else xform_out
+        return xform_out
 
     def ConvShared(
         self,
diff --git a/detectron/modeling/mask_rcnn_heads.py b/detectron/modeling/mask_rcnn_heads.py
index bf76e83..eb7b118 100644
--- a/detectron/modeling/mask_rcnn_heads.py
+++ b/detectron/modeling/mask_rcnn_heads.py
@@ -44,48 +44,202 @@ import detectron.utils.blob as blob_utils
 # Mask R-CNN outputs and losses
 # ---------------------------------------------------------------------------- #
 
-def add_mask_rcnn_outputs(model, blob_in, dim):
-    """Add Mask R-CNN specific outputs: either mask logits or probs."""
-    num_cls = cfg.MODEL.NUM_CLASSES if cfg.MRCNN.CLS_SPECIFIC_MASK else 1
 
-    if cfg.MRCNN.USE_FC_OUTPUT:
-        # Predict masks with a fully connected layer (ignore 'fcn' in the blob
-        # name)
-        dim_fc = int(dim * (cfg.MRCNN.RESOLUTION / cfg.MRCNN.UPSAMPLE_RATIO)**2)
-        blob_out = model.FC(
-            blob_in,
-            'mask_fcn_logits',
-            dim_fc,
-            num_cls * cfg.MRCNN.RESOLUTION**2,
-            weight_init=gauss_fill(0.001),
-            bias_init=const_fill(0.0)
-        )
+def concat_cls_score_bbox_pred(model):
+    # flatten 'bbox_pred_w'
+    # bbox_pred_w has shape (324, 1024), where 324 is (81, 4) memory structure
+    # reshape to (81, 4 * 1024)
+    model.net.Reshape(
+        'bbox_pred_w', ['bbox_pred_w_flat', '_bbox_pred_w_oldshape'],
+        shape=(model.num_classes, -1))
+    cls_score_bbox_pred, _ = model.net.Concat(
+        ['cls_score_w', 'bbox_pred_w_flat'],
+        ['cls_score_bbox_pred', '_cls_score_bbox_pred_split_info'], axis=1)
+    return cls_score_bbox_pred
+
+
+def bbox2mask_weight_transfer(model, class_embed, dim_in, dim_h, dim_out):
+    bbox2mask_type = str(cfg.MRCNN.BBOX2MASK.TYPE)
+
+    def _mlp_activation(model, inputs, outputs):
+        if cfg.MRCNN.BBOX2MASK.USE_LEAKYRELU:
+            model.net.LeakyRelu(inputs, outputs, alpha=0.1)
+        else:
+            model.net.Relu(inputs, outputs)
+
+    if (not bbox2mask_type) or bbox2mask_type == '1_layer':
+        mask_w_flat = model.FC(
+            class_embed, 'mask_fcn_logits_w_flat', dim_in, dim_out,
+            weight_init=('MSRAFill', {}),
+            bias_init=('ConstantFill', {'value': 0.}))
+    elif bbox2mask_type == '2_layer':
+        mlp_l1 = model.FC(
+            class_embed, 'bbox2mask_mlp_l1', dim_in, dim_h,
+            weight_init=('MSRAFill', {}),
+            bias_init=('ConstantFill', {'value': 0.}))
+        _mlp_activation(model, mlp_l1, mlp_l1)
+        mask_w_flat = model.FC(
+            mlp_l1, 'mask_fcn_logits_w_flat', dim_h, dim_out,
+            weight_init=('MSRAFill', {}),
+            bias_init=('ConstantFill', {'value': 0.}))
+    elif bbox2mask_type == '3_layer':
+        mlp_l1 = model.FC(
+            class_embed, 'bbox2mask_mlp_l1', dim_in, dim_h,
+            weight_init=('MSRAFill', {}),
+            bias_init=('ConstantFill', {'value': 0.}))
+        _mlp_activation(model, mlp_l1, mlp_l1)
+        mlp_l2 = model.FC(
+            mlp_l1, 'bbox2mask_mlp_l2', dim_h, dim_h,
+            weight_init=('MSRAFill', {}),
+            bias_init=('ConstantFill', {'value': 0.}))
+        _mlp_activation(model, mlp_l2, mlp_l2)
+        mask_w_flat = model.FC(
+            mlp_l2, 'mask_fcn_logits_w_flat', dim_h, dim_out,
+            weight_init=('MSRAFill', {}),
+            bias_init=('ConstantFill', {'value': 0.}))
     else:
-        # Predict mask using Conv
+        raise ValueError('unknown bbox2mask_type {}'.format(bbox2mask_type))
+
+    # mask_w has shape (num_cls, dim_out, 1, 1)
+    mask_w = model.net.ExpandDims(
+        mask_w_flat, 'mask_fcn_logits_w', dims=[2, 3])
+    return mask_w
+
+
+def cls_agnostic_mlp_branch(model, blob_in, dim_in, num_cls, dim_h=1024):
+    fc_mask_head_type = str(cfg.MRCNN.MLP_MASK_BRANCH_TYPE)
+    dim_out = 1 * cfg.MRCNN.RESOLUTION**2
+
+    if (not fc_mask_head_type) or fc_mask_head_type == '1_layer':
+        raw_mlp_branch = model.FC(
+            blob_in, 'mask_mlp_logits_raw', dim_in, dim_out,
+            weight_init=('GaussianFill', {'std': 0.001}),
+            bias_init=('ConstantFill', {'value': 0.}))
+    elif fc_mask_head_type == '2_layer':
+        mlp_l1 = model.FC(
+            blob_in, 'fc_mask_head_mlp_l1', dim_in, dim_h,
+            weight_init=('MSRAFill', {}),
+            bias_init=('ConstantFill', {'value': 0.}))
+        model.net.Relu(mlp_l1, mlp_l1)
+        raw_mlp_branch = model.FC(
+            mlp_l1, 'mask_mlp_logits_raw', dim_h, dim_out,
+            weight_init=('GaussianFill', {'std': 0.001}),
+            bias_init=('ConstantFill', {'value': 0.}))
+    elif fc_mask_head_type == '3_layer':
+        mlp_l1 = model.FC(
+            blob_in, 'fc_mask_head_mlp_l1', dim_in, dim_h,
+            weight_init=('MSRAFill', {}),
+            bias_init=('ConstantFill', {'value': 0.}))
+        model.net.Relu(mlp_l1, mlp_l1)
+        mlp_l2 = model.FC(
+            mlp_l1, 'fc_mask_head_mlp_l2', dim_h, dim_h,
+            weight_init=('MSRAFill', {}),
+            bias_init=('ConstantFill', {'value': 0.}))
+        model.net.Relu(mlp_l2, mlp_l2)
+        raw_mlp_branch = model.FC(
+            mlp_l2, 'mask_mlp_logits_raw', dim_h, dim_out,
+            weight_init=('GaussianFill', {'std': 0.001}),
+            bias_init=('ConstantFill', {'value': 0.}))
+    else:
+        raise ValueError('unknown fc_mask_head_type {}'.format(fc_mask_head_type))
 
-        # Use GaussianFill for class-agnostic mask prediction; fills based on
-        # fan-in can be too large in this case and cause divergence
-        fill = (
-            cfg.MRCNN.CONV_INIT
-            if cfg.MRCNN.CLS_SPECIFIC_MASK else 'GaussianFill'
-        )
-        blob_out = model.Conv(
-            blob_in,
-            'mask_fcn_logits',
-            dim,
-            num_cls,
-            kernel=1,
-            pad=0,
-            stride=1,
-            weight_init=(fill, {'std': 0.001}),
-            bias_init=const_fill(0.0)
-        )
+    mlp_branch, _ = model.net.Reshape(
+        raw_mlp_branch,
+        ['mask_mlp_logits_reshaped', '_mask_mlp_logits_raw_old_shape'],
+        shape=(-1, 1, cfg.MRCNN.RESOLUTION, cfg.MRCNN.RESOLUTION))
+    if num_cls > 1:
+        mlp_branch = model.net.Tile(
+            mlp_branch, 'mask_mlp_logits_tiled', tiles=num_cls, axis=1)
+
+    return mlp_branch
 
-        if cfg.MRCNN.UPSAMPLE_RATIO > 1:
-            blob_out = model.BilinearInterpolation(
-                'mask_fcn_logits', 'mask_fcn_logits_up', num_cls, num_cls,
-                cfg.MRCNN.UPSAMPLE_RATIO
-            )
+
+def add_mask_rcnn_outputs(model, blob_in, dim):
+    num_cls = cfg.MODEL.NUM_CLASSES if cfg.MRCNN.CLS_SPECIFIC_MASK else 1
+
+    if cfg.MRCNN.BBOX2MASK.BBOX2MASK_ON:
+        # Use weight transfer function iff BBOX2MASK_ON is True
+        # Decide the input to the of weight transfer function
+        #   - Case 1) From a pre-trained embedding vector (e.g. GloVe)
+        #   - Case 2) From the detection weights in the box head
+        if cfg.MRCNN.BBOX2MASK.USE_PRETRAINED_EMBED:
+            # Case 1) From a pre-trained embedding vector (e.g. GloVe)
+            class_embed = cfg.MRCNN.BBOX2MASK.PRETRAINED_EMBED_NAME
+            class_embed_dim = cfg.MRCNN.BBOX2MASK.PRETRAINED_EMBED_DIM
+            # This parameter is meant to be initialized from a pretrained model
+            # instead of learned from scratch. Hence, the default init is HUGE
+            # to cause NaN loss so that the error will not pass silently.
+            model.AddParameter(model.param_init_net.GaussianFill(
+                [], class_embed, shape=[num_cls, class_embed_dim], std=1e12))
+            # Pretrained embedding should be fixed during training (it doesn't
+            # make sense to update them)
+            model.StopGradient(class_embed, class_embed + '_no_grad')
+            class_embed = class_embed + '_no_grad'
+        else:
+            # Case 2) From the detection weights in the box head
+            #   - Subcase a) using cls+box
+            #   - Subcase b) using cls
+            #   - Subcase c) using box
+            # where 'cls' is RoI classification weights 'cls_score_w'
+            # and 'box' is bounding box regression weights 'bbox_pred_w'
+            if (cfg.MRCNN.BBOX2MASK.INCLUDE_CLS_SCORE and
+                    cfg.MRCNN.BBOX2MASK.INCLUDE_BBOX_PRED):
+                # Subcase a) using cls+box
+                concat_cls_score_bbox_pred(model)
+                class_embed = 'cls_score_bbox_pred'
+                class_embed_dim = 1024 + 4096
+            elif cfg.MRCNN.BBOX2MASK.INCLUDE_CLS_SCORE:
+                # Subcase b) using cls
+                class_embed = 'cls_score_w'
+                class_embed_dim = 1024
+            elif cfg.MRCNN.BBOX2MASK.INCLUDE_BBOX_PRED:
+                # Subcase c) using box; 'bbox_pred_w' need to be flattened
+                model.net.Reshape(
+                    'bbox_pred_w', ['bbox_pred_w_flat', '_bbox_pred_w_oldshape'],
+                    shape=(model.num_classes, -1))
+                class_embed = 'bbox_pred_w_flat'
+                class_embed_dim = 4096
+            else:
+                raise ValueError(
+                    'At least one of cfg.MRCNN.BBOX2MASK.INCLUDE_CLS_SCORE and '
+                    'cfg.MRCNN.BBOX2MASK.INCLUDE_BBOX_PRED needs to be True')
+            # Stop the mask gradient to the detection weights if specified
+            if cfg.MRCNN.BBOX2MASK.STOP_DET_W_GRAD:
+                model.StopGradient(class_embed, class_embed + '_no_grad')
+                class_embed = class_embed + '_no_grad'
+
+        # Use weights transfer function to predict mask weights
+        mask_w = bbox2mask_weight_transfer(
+            model, class_embed, dim_in=class_embed_dim, dim_h=dim, dim_out=dim)
+        # Mask prediction with predicted mask weights (no bias term)
+        fcn_branch = model.net.Conv(
+            [blob_in, mask_w], 'mask_fcn_logits', kernel=1, pad=0, stride=1)
+    else:
+        # Not using weights transfer function
+        if cfg.MRCNN.USE_FC_OUTPUT:
+            assert not cfg.MRCNN.JOINT_FCN_MLP_HEAD
+            blob_out = model.FC(
+                blob_in, 'mask_fcn_logits', dim,
+                num_cls * cfg.MRCNN.RESOLUTION**2,
+                weight_init=('GaussianFill', {'std': 0.001}),
+                bias_init=('ConstantFill', {'value': 0.}))
+        else:
+            # If using class-agnostic mask, scale down init to avoid NaN loss
+            init_filler = (
+                cfg.MRCNN.CONV_INIT if cfg.MRCNN.CLS_SPECIFIC_MASK else 'GaussianFill')
+            fcn_branch = model.Conv(
+                blob_in, 'mask_fcn_logits', dim, num_cls, 1, pad=0, stride=1,
+                weight_init=(init_filler, {'std': 0.001}),
+                bias_init=('ConstantFill', {'value': 0.}))
+
+    # Add a complementary MLP branch if specified
+    if cfg.MRCNN.JOINT_FCN_MLP_HEAD:
+        # Use class-agnostic MLP branch, and class-aware FCN branch
+        mlp_branch = cls_agnostic_mlp_branch(
+            model, blob_in, dim_in=dim * cfg.MRCNN.RESOLUTION**2, num_cls=num_cls)
+        blob_out = model.net.Add([mlp_branch, fcn_branch], 'mask_logits')
+    elif not cfg.MRCNN.USE_FC_OUTPUT:
+        blob_out = fcn_branch
 
     if not model.train:  # == if test
         blob_out = model.net.Sigmoid(blob_out, 'mask_fcn_probs')
@@ -154,7 +308,6 @@ def mask_rcnn_fcn_head_v1upXconvs(
             dim_in,
             dim_inner,
             kernel=3,
-            dilation=dilation,
             pad=1 * dilation,
             stride=1,
             weight_init=(cfg.MRCNN.CONV_INIT, {'std': 0.001}),
diff --git a/detectron/modeling/model_builder.py b/detectron/modeling/model_builder.py
index 25ab217..b884dfd 100644
--- a/detectron/modeling/model_builder.py
+++ b/detectron/modeling/model_builder.py
@@ -219,9 +219,12 @@ def build_generic_detection_model(
 
         if model.train:
             loss_gradients = {}
-            for lg in head_loss_gradients.values():
-                if lg is not None:
-                    loss_gradients.update(lg)
+            if cfg.TRAIN.TRAIN_MASK_HEAD_ONLY:
+                loss_gradients.update(head_loss_gradients['mask'])
+            else:
+                for lg in head_loss_gradients.values():
+                    if lg is not None:
+                        loss_gradients.update(lg)
             return loss_gradients
         else:
             return None
@@ -254,7 +257,7 @@ def _add_fast_rcnn_head(
         model, blob_in, dim_in, spatial_scale_in
     )
     fast_rcnn_heads.add_fast_rcnn_outputs(model, blob_frcn, dim_frcn)
-    if model.train:
+    if model.train and not cfg.TRAIN.TRAIN_MASK_HEAD_ONLY:
         loss_gradients = fast_rcnn_heads.add_fast_rcnn_losses(model)
     else:
         loss_gradients = None
@@ -316,8 +319,10 @@ def _add_roi_keypoint_head(
         )
         model.net._net = bbox_net
         loss_gradients = None
-    else:
+    elif not cfg.TRAIN.TRAIN_MASK_HEAD_ONLY:
         loss_gradients = keypoint_rcnn_heads.add_keypoint_losses(model)
+    else:
+        loss_gradients = None
     return loss_gradients
 
 
@@ -388,7 +393,7 @@ def add_training_inputs(model, roidb=None):
             blobs_queue_capacity=cfg.DATA_LOADER.BLOBS_QUEUE_CAPACITY
         )
     orig_num_op = len(model.net._net.op)
-    blob_names = roi_data_minibatch.get_minibatch_blob_names(is_training=True)
+    blob_names = roi_data.minibatch.get_minibatch_blob_names(is_training=True)
     for gpu_id in range(cfg.NUM_GPUS):
         with c2_utils.NamedCudaScope(gpu_id):
             for blob_name in blob_names:
diff --git a/detectron/modeling/rpn_heads.py b/detectron/modeling/rpn_heads.py
index 1f0a9b5..743e402 100644
--- a/detectron/modeling/rpn_heads.py
+++ b/detectron/modeling/rpn_heads.py
@@ -42,12 +42,12 @@ def add_generic_rpn_outputs(model, blob_in, dim_in, spatial_scale_in):
             # CollectAndDistributeFpnRpnProposals also labels proposals when in
             # training mode
             model.CollectAndDistributeFpnRpnProposals()
-        if model.train:
+        if model.train and not cfg.TRAIN.TRAIN_MASK_HEAD_ONLY:
             loss_gradients = FPN.add_fpn_rpn_losses(model)
     else:
         # Not using FPN, add RPN to a single scale
         add_single_scale_rpn_outputs(model, blob_in, dim_in, spatial_scale_in)
-        if model.train:
+        if model.train and not cfg.TRAIN.TRAIN_MASK_HEAD_ONLY:
             loss_gradients = add_single_scale_rpn_losses(model)
     return loss_gradients
 
-- 
2.20.1

